# ğŸ¨ Muma AI - Your Personal Museum Companion

<div align="center">

![Muma AI](banner-muma.png)

### ElevenLabs Worldwide Hackathon Submission

**An AI companion designed to enhance museum visits by helping users understand and explore artworks with ease.**

[Live Demo](https://muma-sigma.vercel.app/) â€¢ [GitHub Repository](https://github.com/ai-muma/appmuma)

</div>

---

## ğŸ“¹ Demo Video

https://github.com/user-attachments/assets/video.mp4

---

## ğŸ¯ Problem Statement

Museum visitors often lack context or background information about the many paintings, sculptures, and art pieces they encounter, making it difficult to fully engage with each work. Traditional museum guides are:
- Limited by group sizes and schedules
- Unable to provide personalized, in-depth conversations
- Not accessible to everyone at all times

**Muma addresses this challenge** by allowing users to take a photo of an artwork, which is then analyzed by AI to provide clear, informative insights about the piece and its creator, enabling a more accessible and enriching cultural experience.

---

## ğŸ‘¥ Team

**Manuel** - AI Engineer/CTO at Desde Arriba  
Building FastAPI agentic systems

**IÃ±igo** - Web3 UX Designer at Puffer Finance  
Focused on liquid restaking

---

## âœ¨ Features

- **ğŸ” AI Vision Recognition**: Instantly identify artworks using OpenAI GPT-4 Vision API
- **ğŸ—£ï¸ Natural Voice Conversations**: Engage in natural dialogue with ElevenLabs Conversational AI
- **ğŸ“š Rich Context**: Get detailed artwork information including history, technique, and fascinating facts
- **ğŸ“¸ Multiple Input Methods**: Upload files or capture images directly from your camera using Web Speech API
- **ğŸ¯ Real-time Analysis**: Receive immediate, accurate artwork identification
- **ğŸŒ Accessible Anywhere**: Works with any artwork, no pre-built database required
- **ğŸ¤ Voice Commands**: Trigger photo capture using voice commands via Web Speech API

---

## ğŸ—ï¸ Architecture

```
User â†’ Voice Command / Image Upload â†’ /api/analyze-artwork â†’ OpenAI Vision API
                                                           â†“
                                              Build Context from Vision Results
                                                           â†“
                                          ElevenLabs Conversation Agent
                                                           â†“
                                          Natural Voice Interaction
```

---

## ğŸ› ï¸ Tech Stack

### Frontend
- **Next.js 16** - React-based framework for production
- **React 19** - Modern UI library
- **TailwindCSS** - Utility-first CSS framework
- **TypeScript** - Type-safe development

### AI & APIs
- **OpenAI GPT-4o Vision API** - Artwork identification and analysis
- **ElevenLabs Conversational AI SDK** - Natural voice conversations
- **Web Speech API** - Voice command recognition for photo capture

### Development Tools
- **CodeRabbit** - AI-powered code review and development assistance
- **BlackBox** - Development tooling
- **Clerk** - User authentication (planned)

---

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18.x or higher
- npm or yarn package manager
- OpenAI API key with GPT-4 Vision access
- ElevenLabs API key and Agent ID

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/ai-muma/appmuma.git
cd appmuma
```

2. **Install dependencies**
```bash
npm install
```

3. **Configure environment variables**

Create a `.env.local` file in the root directory:

```bash
# OpenAI API Key for Vision API
OPENAI_API_KEY=your_openai_api_key_here

# ElevenLabs API Keys
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
NEXT_PUBLIC_AGENT_ID=your_elevenlabs_agent_id_here
```

4. **Get your API keys**

**OpenAI API Key:**
- Visit [OpenAI Platform](https://platform.openai.com/)
- Sign up or log in
- Navigate to API Keys section
- Create a new API key
- Copy and paste into `.env.local`

**ElevenLabs API Key & Agent ID:**
- Visit [ElevenLabs](https://elevenlabs.io/)
- Sign up or log in
- Navigate to your profile/API settings
- Copy your API key
- Create a conversational agent and copy the Agent ID
- Paste both into `.env.local`

5. **Run the development server**
```bash
npm run dev
```

6. **Open your browser**

Navigate to [http://localhost:3000](http://localhost:3000)

---

## ğŸ’¡ How to Use Muma

1. **Capture an Artwork**
   - Click the camera button or use voice commands
   - Upload an image or take a photo directly
   - Ensure the artwork is clearly visible

2. **AI Analysis**
   - OpenAI Vision API identifies the artwork
   - System extracts detailed information
   - Context is prepared for conversation

3. **Start Conversation**
   - Click "Start Conversation" button
   - Allow microphone access in your browser
   - Begin speaking naturally with the AI

4. **Explore & Learn**
   - Ask about the artist's background
   - Inquire about historical context
   - Discuss artistic techniques
   - Learn interesting facts and stories

---

## ğŸ“ Project Structure

```
muma/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ analyze-artwork/
â”‚   â”‚       â””â”€â”€ route.ts              # Backend API endpoint for artwork analysis
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ artwork-uploader.tsx      # Image upload/camera component
â”‚   â”‚   â””â”€â”€ conversation.tsx          # ElevenLabs conversation component
â”‚   â”œâ”€â”€ page.tsx                      # Main application page
â”‚   â”œâ”€â”€ layout.tsx                    # App layout wrapper
â”‚   â””â”€â”€ globals.css                   # Global styles
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ openai.ts                     # OpenAI client configuration
â”‚   â””â”€â”€ mock-artwork-data.ts          # Mock database for testing
â”œâ”€â”€ public/                           # Static assets
â”œâ”€â”€ .env.local                        # Environment variables (create this)
â”œâ”€â”€ package.json                      # Project dependencies
â””â”€â”€ tsconfig.json                     # TypeScript configuration
```

---

## ğŸ”Œ API Endpoints

### POST /api/analyze-artwork

Analyzes an uploaded artwork image using OpenAI Vision API.

**Request:**
```json
{
  "imageData": "data:image/jpeg;base64,..."
}
```

**Response:**
```json
{
  "success": true,
  "identified": true,
  "inDatabase": true,
  "identification": {
    "name": "The Starry Night",
    "artist": "Vincent van Gogh",
    "year": "1889",
    "medium": "Oil on canvas",
    "confidence": "high"
  },
  "artwork": {
    "id": "starry-night-1889",
    "name": "The Starry Night",
    "artist": "Vincent van Gogh",
    "year": "1889",
    "medium": "Oil on canvas",
    "description": "...",
    "conversationContext": "..."
  }
}
```

### GET /api/analyze-artwork

Health check endpoint to verify API availability.

---

## ğŸ¨ Use Cases

- **Museum Visitors**: Get instant, personalized information about any artwork
- **Students**: Learn about art history through interactive conversations
- **Tourists**: Overcome language barriers with accessible art education
- **Art Enthusiasts**: Deepen understanding of techniques and historical context
- **Educators**: Use as a teaching tool for art appreciation

---

## ğŸŒŸ What Makes Muma Special

- **No Pre-built Database Required**: Works with any artwork in any museum
- **Natural Conversations**: Not just facts, but engaging dialogue about art
- **Accessible**: Voice-first interface makes art education available to everyone
- **Instant Recognition**: Fast, accurate artwork identification
- **Rich Context**: Goes beyond basic facts to provide meaningful insights

---

## ğŸ”® Future Enhancements

- [ ] Multi-language support for international museums
- [ ] User authentication and profile management
- [ ] Save favorite artworks and conversation history
- [ ] Offline mode for museums with limited connectivity
- [ ] AR overlay features for enhanced museum experience
- [ ] Integration with museum collection databases
- [ ] Social features to share discoveries with friends
- [ ] Artist comparison and art movement exploration
- [ ] Export conversation transcripts and notes
- [ ] Mobile app for iOS and Android

---

## ğŸ› Troubleshooting

**"Missing OPENAI_API_KEY environment variable"**
- Ensure `.env.local` exists in the root directory
- Verify your OpenAI API key is correctly added
- Restart the development server after adding variables

**"Unable to access camera"**
- Grant camera permissions in your browser settings
- Ensure you're using HTTPS or localhost
- Check if another application is using the camera

**"NEXT_PUBLIC_AGENT_ID not configured"**
- Add your ElevenLabs Agent ID to `.env.local`
- Remember: the variable must start with `NEXT_PUBLIC_` for client-side access
- Restart the development server

**"Microphone not working"**
- Allow microphone access when prompted by the browser
- Check browser permissions in settings
- Ensure no other application is blocking microphone access

---

## ğŸ“ License

MIT License - feel free to use this project for your own museum experiences!

---

## ğŸ™ Credits & Acknowledgments

Built with love for the **ElevenLabs Worldwide Hackathon**

### Technologies
- [Next.js](https://nextjs.org/) - The React Framework for the Web
- [OpenAI](https://openai.com/) - GPT-4 Vision API for artwork identification
- [ElevenLabs](https://elevenlabs.io/) - Conversational AI for natural voice interactions
- [TailwindCSS](https://tailwindcss.com/) - Utility-first CSS framework
- [Web Speech API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API) - Voice command recognition

### Tools
- [CodeRabbit](https://coderabbit.ai/) - AI-powered code review and development assistance
- [BlackBox](https://www.blackbox.ai/) - Development tooling
- [Clerk](https://clerk.com/) - User authentication platform

### Team
Special thanks to our amazing team members who made this project possible!

---

## ğŸ“§ Contact

For questions, feedback, or collaboration opportunities:
- Email: mhernandezb96@gmail.com
- GitHub: [ai-muma/appmuma](https://github.com/ai-muma/appmuma)
- Live Demo: [muma-sigma.vercel.app](https://muma-sigma.vercel.app/)

---

<div align="center">

**Made with â¤ï¸ for art lovers everywhere**

*Enhancing museum visits, one conversation at a time*

</div>
